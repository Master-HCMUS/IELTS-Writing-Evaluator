from __future__ import annotations

import json
from pathlib import Path


def _load_rubric_summary() -> str:
    """Load condensed rubric from docs/rubric/v1/summary.md"""
    rubric_path = Path(__file__).parents[3] / "docs" / "rubric" / "v1" / "summary.md"
    if rubric_path.exists():
        content = rubric_path.read_text(encoding="utf-8")
        # Extract Task 2 section
        if "Task 2 Criteria" in content:
            lines = content.split("\n")
            task2_lines = []
            in_task2 = False
            for line in lines:
                if "Task 2 Criteria" in line:
                    in_task2 = True
                elif "Task 1 Criteria" in line:
                    in_task2 = False
                elif in_task2:
                    task2_lines.append(line)
            return "\n".join(task2_lines[:10])  # Keep concise
    return "Task Response, Coherence & Cohesion, Lexical Resource, Grammatical Range & Accuracy"


def _load_anchors() -> dict:
    """Load anchor exemplars from docs/rubric/v1/anchors.json"""
    anchors_path = Path(__file__).parents[3] / "docs" / "rubric" / "v1" / "anchors.json"
    if anchors_path.exists():
        return json.loads(anchors_path.read_text(encoding="utf-8")).get("task2", {})
    return {}


def get_system_prompt() -> str:
    """System prompt for Task 2 scoring with evidence-first JSON-only constraints.
    
    This prompt is loaded from the optimized best_prompt.txt file generated by the
    prompt optimizer. If the file doesn't exist, falls back to a default prompt.
    
    The prompt includes band descriptors, error calibration, and scoring guidance
    that has been iteratively refined to improve scoring accuracy.
    """
    # Try to load optimized prompt from best_prompt.txt
    best_prompt_path = Path(__file__).parents[3] / "experiments" / "prompt_optimization" / "best_prompt.txt"
    
    if best_prompt_path.exists():
        try:
            optimized_prompt = best_prompt_path.read_text(encoding="utf-8").strip()
            # Strip markdown code blocks if present
            if optimized_prompt.startswith("```") and optimized_prompt.endswith("```"):
                lines = optimized_prompt.split("\n")
                optimized_prompt = "\n".join(lines[1:-1]).strip()
            # Verify it's not empty and looks like a valid prompt
            if optimized_prompt and ("INSTRUCTIONS" in optimized_prompt or "CRITICAL INSTRUCTIONS" in optimized_prompt):
                return optimized_prompt
        except Exception as e:
            # If loading fails, fall back to default
            import logging
            logging.warning(f"Failed to load optimized prompt from {best_prompt_path}: {e}")
    
    # Fallback: generate default prompt with rubric and anchors
    rubric = _load_rubric_summary()
    anchors = _load_anchors()
    
    # Format anchors concisely
    anchor_text = ""
    for criterion, bands in anchors.items():
        if bands:
            anchor_text += f"\n{criterion}:"
            for band, desc in list(bands.items())[:2]:  # Keep small
                anchor_text += f"\n  {band}: {desc}"
    
    return f"""You are an experienced IELTS examiner evaluating Task 2 essays.

CRITICAL INSTRUCTIONS:
1. Respond ONLY in valid JSON format. No explanatory text outside JSON.
2. For each criterion, provide direct verbatim quotes from the essay as evidence.
3. All quotes MUST exist exactly in the essay text.
4. Chain-of-thought reasoning is encouraged but not required. Only structured JSON output.
5. Band scores: 0-9 in 0.5 increments.

RUBRIC (Task 2):
{rubric}

ANCHOR EXAMPLES:{anchor_text}

RESPONSE STRUCTURE:
- per_criterion: array of criterion assessments
  - name: criterion name
  - band: numeric score (0-9, increments of 0.5)
  - evidence_quotes: array of verbatim quotes (max 3)
  - errors: array of error objects (max 10)
    - span: exact problematic text
    - type: "grammar" | "lexical" | "coherence" | "task" | "other"
    - fix: brief correction
  - suggestions: array of specific improvements (max 5, each â‰¤200 chars)
- overall: overall band (0-9, increments of 0.5)

Ensure all text spans are copied exactly from the essay."""


def get_user_prompt(essay: str, question: str | None = None) -> str:
    """User prompt with the question (if provided) and essay to score."""
    if question:
        return f"""Task 2 Question:
{question}

Score this IELTS Task 2 essay according to the rubric, ensuring alignment with the question:

{essay}

Provide your assessment in the specified JSON format."""
    # Fallback (legacy)
    return f"""Score this IELTS Task 2 essay according to the rubric:

{essay}

Provide your assessment in the specified JSON format."""


def get_response_schema() -> dict:
    """Simplified schema hint for the model."""
    return {
        "type": "object",
        "properties": {
            "per_criterion": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "name": {"type": "string"},
                        "band": {"type": "number"},
                        "evidence_quotes": {"type": "array", "items": {"type": "string"}},
                        "errors": {"type": "array"},
                        "suggestions": {"type": "array", "items": {"type": "string"}}
                    }
                }
            },
            "overall": {"type": "number"}
        }
    }
