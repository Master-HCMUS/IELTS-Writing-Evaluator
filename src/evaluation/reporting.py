from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any

import pandas as pd


@dataclass
class ReportConfig:
    output_dir: Path
    include_plots: bool = True


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def save_artifacts(preds: pd.DataFrame, metrics: Dict[str, Any], cfg: ReportConfig, timing_stats: dict = None) -> Dict[str, Path]:
    date_prefix = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    out_dir = cfg.output_dir if cfg.output_dir.is_absolute() else Path.cwd() / cfg.output_dir
    out_dir = out_dir / date_prefix
    _ensure_dir(out_dir)

    # Files
    preds_csv = out_dir / "predictions.csv"
    metrics_json = out_dir / "metrics.json"
    report_md = out_dir / "report.md"

    preds.to_csv(preds_csv, index=False)
    metrics_json.write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding="utf-8")

    # Basic Markdown report
    lines = [
        "# IELTS Task 2 Evaluation Report",
        "",
        f"Date (UTC): {datetime.now(timezone.utc).isoformat()}",
        "",
        "## Configuration",
        f"- Dataset: chillies/IELTS-writing-task-2-evaluation",
        f"- Split: test",
        f"- N: {len(preds)}",
        "",
    ]
    # Timing summary
    if timing_stats and timing_stats.get("num_samples", 0) > 0:
        lines += [
            "## Timing Summary",
            f"- Total time:   {timing_stats['total_time']:.3f} sec for {timing_stats['num_samples']} samples",
            f"- Longest run:  {timing_stats['max_time']:.3f} sec",
            f"- Fastest run:  {timing_stats['min_time']:.3f} sec",
            f"- Average time: {timing_stats['avg_time']:.3f} sec",
            "",
        ]

    lines += [
        "## Metrics Summary",
        "### Overall Band Score",
        f"- QWK: {metrics['overall']['qwk']:.3f}",
        f"- MAE: {metrics['overall']['mae']:.3f}",
        f"- Within 0.5: {metrics['overall']['within_point5']:.3f}",
        "",
        "### Rubric Scores",
        *[f"**{rubric.upper()}**: QWK={metrics['rubrics'][rubric]['qwk']:.3f}, MAE={metrics['rubrics'][rubric]['mae']:.3f}, Within 0.5={metrics['rubrics'][rubric]['within_point5']:.3f}" 
          for rubric in ['tr', 'cc', 'lr', 'gra'] if rubric in metrics.get('rubrics', {})],
        "",
        "### Other Metrics",
        f"- Dispersion mean / p50 / p95: {metrics['dispersion']['mean']:.3f} / {metrics['dispersion']['p50']:.3f} / {metrics['dispersion']['p95']:.3f}",
        f"- Low-confidence rate (>0.5): {metrics['dispersion']['low_conf_rate']:.3f}",
        f"- Corr(pred, word_count): {metrics['correlations']['pred_vs_word_count']}",
        "",
        "## Notes",
        "- Report generated by evaluation.runner using the reusable scorer pipeline.",
    ]

    report_md.write_text("\n".join(lines), encoding="utf-8")

    return {
        "predictions_csv": preds_csv,
        "metrics_json": metrics_json,
        "report_md": report_md,
    }
