from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any

import pandas as pd


@dataclass
class ReportConfig:
    output_dir: Path
    include_plots: bool = True
    title: str = "IELTS Task 2 Evaluation Report"
    notes: str | None = None
    dataset_name: str | None = None
    split: str | None = None
    num_samples: int | None = None
    use_rubric_pipeline: bool = False
    api_provider: str = "azure"


def _ensure_dir(p: Path) -> None:
    p.mkdir(parents=True, exist_ok=True)


def save_artifacts(preds: pd.DataFrame, metrics: Dict[str, Any], cfg: ReportConfig) -> Dict[str, Path]:
    date_prefix = datetime.now(timezone.utc).strftime("%Y-%m-%d")
    out_dir = cfg.output_dir if cfg.output_dir.is_absolute() else Path.cwd() / cfg.output_dir
    out_dir = out_dir / date_prefix
    _ensure_dir(out_dir)

    # Files
    preds_csv = out_dir / "predictions.csv"
    metrics_json = out_dir / "metrics.json"
    report_md = out_dir / "report.md"

    preds.to_csv(preds_csv, index=False)
    metrics_json.write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding="utf-8")

    # Basic Markdown report
    lines = [
        f"# {cfg.title}",
        "",
        f"Date (UTC): {datetime.now(timezone.utc).isoformat()}",
        "",
        "## Configuration",
        f"- Dataset: {cfg.dataset_name or 'chillies/IELTS-writing-task-2-evaluation'}",
        f"- Split: {cfg.split or 'test'}",
        f"- N: {cfg.num_samples or len(preds)}",
        f"- Pipeline: {'Rubric-specific' if cfg.use_rubric_pipeline else 'Standard 3-pass'}",
        f"- API Provider: {cfg.api_provider}",
        "",
        "## Metrics Summary",
        "### Overall Band Score",
        f"- QWK: {metrics['overall']['qwk']:.3f}",
        f"- MAE: {metrics['overall']['mae']:.3f}",
        f"- Within 0.5: {metrics['overall']['within_point5']:.3f}",
        "",
        "### Rubric Scores",
        *[f"**{rubric.upper()}**: QWK={metrics['rubrics'][rubric]['qwk']:.3f}, MAE={metrics['rubrics'][rubric]['mae']:.3f}, Within 0.5={metrics['rubrics'][rubric]['within_point5']:.3f}" 
          for rubric in ['tr', 'cc', 'lr', 'gra'] if rubric in metrics.get('rubrics', {})],
        "",
        "### Other Metrics",
        f"- Dispersion mean / p50 / p95: {metrics['dispersion']['mean']:.3f} / {metrics['dispersion']['p50']:.3f} / {metrics['dispersion']['p95']:.3f}",
        f"- Low-confidence rate (>0.5): {metrics['dispersion']['low_conf_rate']:.3f}",
        f"- Corr(pred, word_count): {metrics['correlations']['pred_vs_word_count']}",
        "",
        "## Notes",
        "- Report generated by evaluation.runner using the reusable scorer pipeline.",
    ]
    
    # Add custom notes if provided
    if cfg.notes:
        lines.extend([
            "",
            "## Additional Notes",
            cfg.notes,
        ])

    report_md.write_text("\n".join(lines), encoding="utf-8")

    return {
        "predictions_csv": preds_csv,
        "metrics_json": metrics_json,
        "report_md": report_md,
    }
