{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cb22fc",
   "metadata": {},
   "source": [
    "# Notebook: IELTS Writing Task 2 Scoring with Mistral-7B-Instruct-v0.2 (Kaggle)\n",
    "\n",
    "This notebook demonstrates how to use the Mistral-7B-Instruct-v0.2 model to automatically score IELTS Writing Task 2 essays. The pipeline uses standardized schemas, validation, prompts, and batch evaluation as in the reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7706fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Install and import required libraries\n",
    "!pip install --upgrade pip\n",
    "!pip install -q transformers accelerate torch jsonschema\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import base64\n",
    "from jsonschema import validate, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c43cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Load Mistral-7B-Instruct model directly from Hugging Face\n",
    "import os\n",
    "\n",
    "MODEL_NAME = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "TOKENIZER_DIR = '/kaggle/working/mistral-7b-instruct-v0.2-tokenizer-cache'\n",
    "MODEL_DIR = '/kaggle/working/mistral-7b-instruct-v0.2-cache'\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_Token\")\n",
    "\n",
    "\n",
    "# Load Hugging Face token from environment variable\n",
    "if not hf_token:\n",
    "    raise ValueError(\"Please set the HF_TOKEN environment variable with your Hugging Face access token.\")\n",
    "\n",
    "if os.path.exists(MODEL_DIR) and os.path.exists(TOKENIZER_DIR):\n",
    "    print('Loading model and tokenizer from local cache...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_DIR,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    print('Downloading model and tokenizer from Hugging Face...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True, use_auth_token=hf_token)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map='auto',\n",
    "        trust_remote_code=True,\n",
    "        use_auth_token=hf_token\n",
    "    )\n",
    "    tokenizer.save_pretrained(TOKENIZER_DIR)\n",
    "    # model.save_pretrained(MODEL_DIR)  # Skipped to avoid disk full error on Kaggle\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section X: Call model with fixed IELTS Task 2 prompt (using loaded model) (for debugging)\n",
    "\n",
    "fixed_prompt = (\n",
    "    \"You are an IELTS Writing Task 2 examiner. \"\n",
    "    \"Score the following essay (0-9) and provide feedback for each criterion: Task Response, Coherence and Cohesion, Lexical Resource, Grammatical Range and Accuracy. \"\n",
    "    \"Return ONLY a valid JSON object with this structure (do not copy the values):\\n\"\n",
    "    \"{\\n\"\n",
    "    \"  \\\"overall\\\": <float>,\\n\"\n",
    "    \"  \\\"per_criterion\\\": {\\n\"\n",
    "    \"    \\\"Task Response\\\": <float>,\\n\"\n",
    "    \"    \\\"Coherence and Cohesion\\\": <float>,\\n\"\n",
    "    \"    \\\"Lexical Resource\\\": <float>,\\n\"\n",
    "    \"    \\\"Grammatical Range and Accuracy\\\": <float>\\n\"\n",
    "    \"  },\\n\"\n",
    "    \"  \\\"feedback\\\": {\\n\"\n",
    "    \"    \\\"Task Response\\\": <string>,\\n\"\n",
    "    \"    \\\"Coherence and Cohesion\\\": <string>,\\n\"\n",
    "    \"    \\\"Lexical Resource\\\": <string>,\\n\"\n",
    "    \"    \\\"Grammatical Range and Accuracy\\\": <string>\\n\"\n",
    "    \"  }\\n\"\n",
    "    \"}\\n\"\n",
    "    \"Now score the following essay and generate your own scores and detailed feedback (do not leave any feedback field empty):\\n\"\n",
    "    \"Question: Many people believe that social networking sites have a huge negative impact on both individuals and society. To what extent do you agree or disagree?\\n\"\n",
    "    \"Essay: Many people believe that social networking sites, such as Facebook, have had a negative impact on individuals and society. While these platforms offer opportunities for communication and information sharing, I agree that their overall effect has been more harmful than beneficial.\\n\\nFirstly, social networking sites can lead to addiction and reduced productivity. Many users spend hours scrolling through feeds, which distracts them from work, study, or real-life relationships. This excessive use can result in poor academic or job performance and even mental health issues such as anxiety and depression.\\n\\nSecondly, these platforms often facilitate the spread of misinformation and cyberbullying. False news can go viral quickly, misleading large numbers of people. Moreover, the anonymity provided by social media allows some individuals to harass or bully others without facing real consequences.\\n\\nHowever, it is important to acknowledge that social networking sites also have positive aspects. They help people stay connected with friends and family, especially those living far away, and can be valuable tools for professional networking and learning.\\n\\nIn conclusion, although social networking sites have some benefits, I believe their negative impacts on productivity, mental health, and social harmony outweigh the positives. Therefore, individuals and governments should take steps to minimize these harms.\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(fixed_prompt, return_tensors='pt').to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "raw_result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Raw model output:\\n\", raw_result)\n",
    "\n",
    "import json\n",
    "import re\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "# Define the expected JSON schema\n",
    "schema = {\n",
    "    'type': 'object',\n",
    "    'properties': {\n",
    "        'overall': {'type': 'number'},\n",
    "        'per_criterion': {'type': 'object'},\n",
    "        'feedback': {'type': 'object'}\n",
    "    },\n",
    "    'required': ['overall', 'per_criterion', 'feedback']\n",
    "}\n",
    "\n",
    "def extract_final_json_block(text):\n",
    "    \"\"\"\n",
    "    Extract the final IELTS scoring JSON block from a long string.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full string containing instructions, essay, and JSON.\n",
    "\n",
    "    Returns:\n",
    "        dict: Parsed JSON object with 'overall', 'per_criterion', and 'feedback'.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no valid JSON block is found.\n",
    "    \"\"\"\n",
    "    matches = re.findall(r'\\{(?:[^{}]|(?R))*\\}', text, re.DOTALL)\n",
    "\n",
    "    # Try parsing each block from the end\n",
    "    for candidate in reversed(matches):\n",
    "        # Skip blocks with placeholders\n",
    "        if \"<\" in candidate or \">\" in candidate:\n",
    "            continue\n",
    "        try:\n",
    "            parsed = json.loads(candidate)\n",
    "            if all(k in parsed for k in [\"overall\", \"per_criterion\", \"feedback\"]):\n",
    "                return parsed\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "    raise ValueError(\"No valid JSON block found.\")\n",
    "\n",
    "actual_result = extract_final_json_block(raw_result)\n",
    "print(\"Parsed and validated JSON result:\\n\", actual_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Define input schema based on score_request.v1.json\n",
    "score_request_schema = {\n",
    "    '$schema': 'https://json-schema.org/draft/2020-12/schema',\n",
    "    '$id': 'score_request.v1.json',\n",
    "    'title': 'ScoreRequestV1',\n",
    "    'type': 'object',\n",
    "    'additionalProperties': False,\n",
    "    'properties': {\n",
    "        'task_type': {'type': 'string', 'enum': ['task1', 'task2']},\n",
    "        'essay': {'type': 'string', 'minLength': 1, 'maxLength': 20000},\n",
    "        'question': {'type': 'string', 'minLength': 5, 'maxLength': 1000},\n",
    "        'image_base64': {'type': 'string'},\n",
    "        'options': {\n",
    "            'type': 'object',\n",
    "            'additionalProperties': False,\n",
    "            'properties': {\n",
    "                'max_evidence': {'type': 'integer', 'minimum': 1, 'maximum': 3}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'required': ['task_type', 'essay'],\n",
    "    'allOf': [\n",
    "        {\n",
    "            'if': {'properties': {'task_type': {'const': 'task1'}}},\n",
    "            'then': {'required': ['essay']}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def validate_score_request(data):\n",
    "    try:\n",
    "        validate(instance=data, schema=score_request_schema)\n",
    "        print('Input is valid according to schema.')\n",
    "        return True\n",
    "    except ValidationError as e:\n",
    "        print(f'Schema error: {e}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a30ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Preprocess input data (essay, question, image_base64)\n",
    "def preprocess_input(data):\n",
    "    essay = data.get('essay', '')\n",
    "    question = data.get('question', '')\n",
    "    image = None\n",
    "    if data.get('image_base64'):\n",
    "        try:\n",
    "            image = base64.b64decode(data['image_base64'])\n",
    "        except Exception:\n",
    "            image = None\n",
    "    return essay, question, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e17d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_task2_prompt(essay, question):\n",
    "    return (\n",
    "        \"You are an IELTS Writing Task 2 examiner. \"\n",
    "        \"Score the following essay (0-9) and provide feedback for each criterion: Task Response, Coherence and Cohesion, Lexical Resource, Grammatical Range and Accuracy. \"\n",
    "        \"Return ONLY a valid JSON object with this structure (replace <float> and <string> with your own values, do not copy them):\\n\"\n",
    "        \"{\\n\"\n",
    "        \"  \\\"overall\\\": <float>,\\n\"\n",
    "        \"  \\\"per_criterion\\\": {\\n\"\n",
    "        \"    \\\"Task Response\\\": <float>,\\n\"\n",
    "        \"    \\\"Coherence and Cohesion\\\": <float>,\\n\"\n",
    "        \"    \\\"Lexical Resource\\\": <float>,\\n\"\n",
    "        \"    \\\"Grammatical Range and Accuracy\\\": <float>\\n\"\n",
    "        \"  },\\n\"\n",
    "        \"  \\\"feedback\\\": {\\n\"\n",
    "        \"    \\\"Task Response\\\": <string>,\\n\"\n",
    "        \"    \\\"Coherence and Cohesion\\\": <string>,\\n\"\n",
    "        \"    \\\"Lexical Resource\\\": <string>,\\n\"\n",
    "        \"    \\\"Grammatical Range and Accuracy\\\": <string>\\n\"\n",
    "        \"  }\\n\"\n",
    "        \"}\\n\"\n",
    "        \"Now score the following essay and generate your own scores and detailed feedback (do not leave any feedback field empty):\\n\"\n",
    "        \"Question: \" + question + \"\\n\"\n",
    "        \"Essay: \" + essay\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_ielts_task2_qwen2(essay, question, model, tokenizer, max_new_tokens=512):\n",
    "    import torch\n",
    "    import re\n",
    "    import json\n",
    "    prompt = get_standard_task2_prompt(essay, question)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    return extract_final_json_block(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Test scoring with sample data\n",
    "sample_data = {\n",
    "    'task_type': 'task2',\n",
    "    'essay': \"\"\"Many people believe that social networking sites, such as Facebook, have had a negative impact on individuals and society. While these platforms offer opportunities for communication and information sharing, I agree that their overall effect has been more harmful than beneficial.\\n\\nFirstly, social networking sites can lead to addiction and reduced productivity. Many users spend hours scrolling through feeds, which distracts them from work, study, or real-life relationships. This excessive use can result in poor academic or job performance and even mental health issues such as anxiety and depression.\\n\\nSecondly, these platforms often facilitate the spread of misinformation and cyberbullying. False news can go viral quickly, misleading large numbers of people. Moreover, the anonymity provided by social media allows some individuals to harass or bully others without facing real consequences.\\n\\nHowever, it is important to acknowledge that social networking sites also have positive aspects. They help people stay connected with friends and family, especially those living far away, and can be valuable tools for professional networking and learning.\\n\\nIn conclusion, although social networking sites have some benefits, I believe their negative impacts on productivity, mental health, and social harmony outweigh the positives. Therefore, individuals and governments should take steps to minimize these harms.\"\"\",\n",
    "    'question': 'Many people believe that social networking sites have a huge negative impact on both individuals and society. To what extent do you agree or disagree?',\n",
    "    'options': {'max_evidence': 2}\n",
    "}\n",
    "\n",
    "if validate_score_request(sample_data):\n",
    "    essay, question, _ = preprocess_input(sample_data)\n",
    "    result = score_ielts_task2_qwen2(essay, question, model, tokenizer)\n",
    "    print(\"Scoring result:\")\n",
    "    print(result)\n",
    "else:\n",
    "    result = {'error': 'Invalid input.'}\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Display scoring result\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Batch test with sample dataset (simulated)\n",
    "import pandas as pd\n",
    "\n",
    "# Simulated sample dataset\n",
    "dataset = [\n",
    "    {\n",
    "        'task_type': 'task2',\n",
    "        'essay': \"\"\"Some people think that the best way to increase road safety is to increase the minimum legal age for driving cars or riding motorbikes. While raising the age limit may reduce accidents among young drivers, I believe that education and stricter law enforcement are more effective solutions.\\n\\nFirstly, increasing the legal age does not guarantee that all new drivers will be responsible. Many accidents are caused by reckless behavior, which can occur at any age. Instead, comprehensive driver education programs can teach young people about the dangers of speeding, drunk driving, and distracted driving.\\n\\nSecondly, strict enforcement of traffic laws, such as penalties for using mobile phones while driving or not wearing seat belts, can deter dangerous behavior. Regular road safety campaigns and random checks can also raise awareness and encourage compliance.\\n\\nIn conclusion, while raising the minimum driving age might have some effect, I believe that education and law enforcement are more effective ways to improve road safety for everyone.\"\"\",\n",
    "        'question': 'Some people think that the best way to increase road safety is to increase the minimum legal age for driving cars or riding motorbikes. To what extent do you agree or disagree?',\n",
    "        'options': {'max_evidence': 2},\n",
    "    },\n",
    "    {\n",
    "        'task_type': 'task2',\n",
    "        'essay': \"\"\"Some people believe that unpaid community service should be a compulsory part of high school programmes. I agree that students can benefit greatly from such experiences, but I do not think it should be mandatory.\\n\\nOn the one hand, volunteering helps students develop important life skills such as teamwork, communication, and empathy. It also allows them to contribute positively to their communities and gain a sense of responsibility.\\n\\nOn the other hand, making community service compulsory may lead to resentment among students who are not genuinely interested. It could also place additional pressure on those who are already struggling with academic demands.\\n\\nIn conclusion, while community service is valuable, I believe it should be encouraged rather than required in high school programmes.\"\"\",\n",
    "        'question': 'Some people believe that unpaid community service should be a compulsory part of high school programmes (for example, working for a charity, improving the neighbourhood or teaching sports to younger children). To what extent do you agree or disagree?',\n",
    "        'options': {'max_evidence': 2},\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if validate_score_request(row):\n",
    "        essay, question, _ = preprocess_input(row)\n",
    "        result = score_ielts_task2_qwen2(essay, question, model, tokenizer)\n",
    "        print(f\"Result for sample {i}:\")\n",
    "        print(result)\n",
    "        results.append({'index': i, 'result': result})\n",
    "    else:\n",
    "        print(f\"Result for sample {i}: Invalid input.\")\n",
    "        results.append({'index': i, 'result': {'error': 'Invalid input.'}})\n",
    "\n",
    "# Display batch results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09727301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Section 9: Ensure output matches score_response.v1.json format\n",
    "# def validate_score_response_format(response):\n",
    "#     schema = {\n",
    "#         'type': 'object',\n",
    "#         'properties': {\n",
    "#             'overall': {'type': 'number'},\n",
    "#             'per_criterion': {'type': 'object'},\n",
    "#             'feedback': {'type': 'object'}\n",
    "#         },\n",
    "#         'required': ['overall', 'per_criterion', 'feedback']\n",
    "#     }\n",
    "#     try:\n",
    "#         validate(instance=response, schema=schema)\n",
    "#         return True\n",
    "#     except ValidationError as e:\n",
    "#         print(f'Output does not match required format: {e}')\n",
    "#         return False\n",
    "\n",
    "# # Check batch results\n",
    "# for r in results:\n",
    "#     if 'error' in r['result']:\n",
    "#         print(f\"Index {r['index']}: Invalid model output: {r['result']['error']}\")\n",
    "#     else:\n",
    "#         print(f\"Index {r['index']}:\", validate_score_response_format(r['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Load and normalize chillies/IELTS-writing-task-2-evaluation dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = 'chillies/IELTS-writing-task-2-evaluation'\n",
    "df = load_dataset(dataset_name, split='test').to_pandas()\n",
    "# Normalize columns\n",
    "if 'band' in df.columns:\n",
    "    df = df.rename(columns={'band': 'band_true'})\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "df['word_count'] = df['essay'].astype(str).str.split().map(len)\n",
    "df = df[['id', 'prompt', 'essay', 'band_true', 'word_count']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Batch scoring on the full test set with Qwen2-7B\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    data = {\n",
    "        'task_type': 'task2',\n",
    "        'essay': row['essay'],\n",
    "        'question': row['prompt'],\n",
    "        'options': {'max_evidence': 2}\n",
    "    }\n",
    "    if validate_score_request(data):\n",
    "        essay, question, _ = preprocess_input(data)\n",
    "        result = score_ielts_task2_qwen2(essay, question, model, tokenizer)\n",
    "        band_pred = result.get('overall', None)\n",
    "    else:\n",
    "        band_pred = None\n",
    "    results.append({'id': row['id'], 'band_true': row['band_true'], 'band_pred': band_pred})\n",
    "\n",
    "df_pred = pd.DataFrame(results)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accdeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12: Compute evaluation metrics MAE, RMSE, QWK, Pearson, Spearman\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "def safe_round(x):\n",
    "    try:\n",
    "        return round(float(x))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_pred = df_pred.dropna(subset=['band_pred']).copy()\n",
    "df_pred['band_pred_round'] = df_pred['band_pred'].map(safe_round)\n",
    "\n",
    "mae = mean_absolute_error(df_pred['band_true'], df_pred['band_pred'])\n",
    "rmse = mean_squared_error(df_pred['band_true'], df_pred['band_pred'], squared=False)\n",
    "qwk = cohen_kappa_score(df_pred['band_true'], df_pred['band_pred_round'], weights='quadratic')\n",
    "pearson = pearsonr(df_pred['band_true'], df_pred['band_pred'])[0]\n",
    "spearman = spearmanr(df_pred['band_true'], df_pred['band_pred'])[0]\n",
    "\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'QWK: {qwk:.4f}')\n",
    "print(f'Pearson: {pearson:.4f}')\n",
    "print(f'Spearman: {spearman:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb294749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13: Display comparison table and plots of predictions and ground truth\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Show table\n",
    "subset = df_pred[['id', 'band_true', 'band_pred', 'band_pred_round']].head(20)\n",
    "print(subset)\n",
    "# Bar plot: predicted vs. true band scores\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.bar(subset['id']-0.2, subset['band_true'], width=0.4, label='True Band', color='skyblue')\n",
    "plt.bar(subset['id']+0.2, subset['band_pred_round'], width=0.4, label='Predicted Band', color='orange')\n",
    "plt.xlabel('Sample ID')\n",
    "plt.ylabel('Band Score')\n",
    "plt.title('True vs. Predicted Band Scores (First 20 Samples)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Scatter plot: all predictions\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x='band_true', y='band_pred', data=df_pred, alpha=0.5)\n",
    "plt.plot([df_pred['band_true'].min(), df_pred['band_true'].max()], [df_pred['band_true'].min(), df_pred['band_true'].max()], 'r--')\n",
    "plt.xlabel('True Band')\n",
    "plt.ylabel('Predicted Band')\n",
    "plt.title('Scatter Plot: True vs. Predicted Band (All Samples)')\n",
    "plt.show()\n",
    "# Confusion matrix (rounded)\n",
    "cm = confusion_matrix(df_pred['band_true'], df_pred['band_pred_round'])\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=sorted(df_pred['band_true'].unique()), yticklabels=sorted(df_pred['band_true'].unique()))\n",
    "plt.xlabel('Predicted Band (rounded)')\n",
    "plt.ylabel('True Band')\n",
    "plt.title('Confusion Matrix: True vs. Predicted Band (Rounded)')\n",
    "plt.show()\n",
    "# Summary statistics\n",
    "print('Summary statistics:')\n",
    "print(df_pred[['band_true', 'band_pred']].describe())\n",
    "# Table of evaluation metrics\n",
    "import pandas as pd\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': ['MAE', 'RMSE', 'QWK', 'Pearson', 'Spearman'],\n",
    "    'Value': [mae, rmse, qwk, pearson, spearman]\n",
    "})\n",
    "print('Evaluation Metrics:')\n",
    "print(metrics_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 14: Save test set prediction results to file\n",
    "output_path = '/kaggle/working/ielts_task2_predictions.csv'\n",
    "df_pred.to_csv(output_path, index=False)\n",
    "print(f\"Saved predictions to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
