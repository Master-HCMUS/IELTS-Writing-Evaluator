{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32cb22fc",
   "metadata": {},
   "source": [
    "# Notebook: IELTS Writing Task 2 Scoring with Qwen2-7B (Kaggle)\n",
    "\n",
    "This notebook demonstrates how to use the Qwen2-7B-Instruct model to automatically score IELTS Writing Task 2 essays. The pipeline uses standardized schemas, validation, prompts, and batch evaluation as in the reference system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7706fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Install and import required libraries\n",
    "!pip install --upgrade pip\n",
    "!pip install -q transformers accelerate torch jsonschema\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import base64\n",
    "from jsonschema import validate, ValidationError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c43cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Load Qwen2-7B model with local cache\n",
    "import os\n",
    "\n",
    "MODEL_NAME = 'Qwen/Qwen2-7B'\n",
    "MODEL_DIR = '/kaggle/working/qwen2-7b-cache'\n",
    "TOKENIZER_DIR = '/kaggle/working/qwen2-7b-tokenizer-cache'\n",
    "\n",
    "if os.path.exists(MODEL_DIR) and os.path.exists(TOKENIZER_DIR):\n",
    "    print('Loading model and tokenizer from local cache...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_DIR,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "else:\n",
    "    print('Downloading model and tokenizer from Hugging Face...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer.save_pretrained(TOKENIZER_DIR)\n",
    "    model.save_pretrained(MODEL_DIR)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Define input schema based on score_request.v1.json\n",
    "score_request_schema = {\n",
    "    '$schema': 'https://json-schema.org/draft/2020-12/schema',\n",
    "    '$id': 'score_request.v1.json',\n",
    "    'title': 'ScoreRequestV1',\n",
    "    'type': 'object',\n",
    "    'additionalProperties': False,\n",
    "    'properties': {\n",
    "        'task_type': {'type': 'string', 'enum': ['task1', 'task2']},\n",
    "        'essay': {'type': 'string', 'minLength': 1, 'maxLength': 20000},\n",
    "        'question': {'type': 'string', 'minLength': 5, 'maxLength': 1000},\n",
    "        'image_base64': {'type': 'string'},\n",
    "        'options': {\n",
    "            'type': 'object',\n",
    "            'additionalProperties': False,\n",
    "            'properties': {\n",
    "                'max_evidence': {'type': 'integer', 'minimum': 1, 'maximum': 3}\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'required': ['task_type', 'essay'],\n",
    "    'allOf': [\n",
    "        {\n",
    "            'if': {'properties': {'task_type': {'const': 'task1'}}},\n",
    "            'then': {'required': ['essay']}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def validate_score_request(data):\n",
    "    try:\n",
    "        validate(instance=data, schema=score_request_schema)\n",
    "        print('Input is valid according to schema.')\n",
    "        return True\n",
    "    except ValidationError as e:\n",
    "        print(f'Schema error: {e}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a30ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Preprocess input data (essay, question, image_base64)\n",
    "def preprocess_input(data):\n",
    "    essay = data.get('essay', '')\n",
    "    question = data.get('question', '')\n",
    "    image = None\n",
    "    if data.get('image_base64'):\n",
    "        try:\n",
    "            image = base64.b64decode(data['image_base64'])\n",
    "        except Exception:\n",
    "            image = None\n",
    "    return essay, question, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Scoring function using Qwen2-7B\n",
    "def get_standard_task2_prompt(essay, question):\n",
    "    return (\n",
    "        f'You are an IELTS Writing Task 2 examiner.\\n'\n",
    "        f'Score the following essay according to the IELTS rubric (0-9) and provide feedback for each criterion (Task Response, Coherence and Cohesion, Lexical Resource, Grammatical Range and Accuracy).\\n'\n",
    "        f'Question: {question}\\nEssay: {essay}\\n'\n",
    "        f'Return a JSON object with keys: overall, per_criterion (dict), feedback (dict).'\n",
    ")\n",
    "\n",
    "def score_ielts_task2_qwen2(essay, question, model, tokenizer, max_new_tokens=128):  # reduced for Kaggle\n",
    "    prompt = get_standard_task2_prompt(essay, question)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
    "    result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    import re\n",
    "    match = re.search(r'\\{.*\\}', result, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group(0))\n",
    "        except Exception:\n",
    "            return {'error': 'Could not parse JSON from model output.'}\n",
    "    return {'error': 'No JSON found in model output.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde2620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Test scoring with sample data\n",
    "sample_data = {\n",
    "    'task_type': 'task2',\n",
    "    'essay': 'In todayâ€™s world, technology has become an integral part of our lives. Some people believe that it has improved our quality of life, while others think it has made life more complicated. Discuss both views and give your own opinion.',\n",
    "    'question': 'Some people believe that technology has improved our quality of life, while others think it has made life more complicated. Discuss both views and give your own opinion.',\n",
    "    'options': {'max_evidence': 2}\n",
    "}\n",
    "\n",
    "if validate_score_request(sample_data):\n",
    "    essay, question, _ = preprocess_input(sample_data)\n",
    "    result = score_ielts_task2_qwen2(essay, question, model, tokenizer)\n",
    "else:\n",
    "    result = {'error': 'Invalid input.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1d2d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Display scoring result\n",
    "import pprint\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d4642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Batch test with sample dataset (simulated)\n",
    "import pandas as pd\n",
    "\n",
    "# Simulated sample dataset (replace with real file if available)\n",
    "dataset = [\n",
    "    {\n",
    "        'task_type': 'task2',\n",
    "        'essay': 'Some people think that the best way to increase road safety is to increase the minimum legal age for driving cars or riding motorbikes. To what extent do you agree or disagree?',\n",
    "        'question': 'Some people think that the best way to increase road safety is to increase the minimum legal age for driving cars or riding motorbikes. To what extent do you agree or disagree?',\n",
    "        'options': {'max_evidence': 2}\n",
    "    },\n",
    "    {\n",
    "        'task_type': 'task2',\n",
    "        'essay': 'Many people believe that social networking sites have a huge negative impact on both individuals and society. To what extent do you agree or disagree?',\n",
    "        'question': 'Many people believe that social networking sites have a huge negative impact on both individuals and society. To what extent do you agree or disagree?',\n",
    "        'options': {'max_evidence': 2}\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if validate_score_request(row):\n",
    "        essay, question, _ = preprocess_input(row)\n",
    "        result = score_ielts_task2_qwen2(essay, question, model, tokenizer)\n",
    "        results.append({'index': i, 'result': result})\n",
    "    else:\n",
    "        results.append({'index': i, 'result': {'error': 'Invalid input.'}})\n",
    "\n",
    "# Display batch results\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09727301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Ensure output matches score_response.v1.json format\n",
    "def validate_score_response_format(response):\n",
    "    schema = {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'overall': {'type': 'number'},\n",
    "            'per_criterion': {'type': 'object'},\n",
    "            'feedback': {'type': 'object'}\n",
    "        },\n",
    "        'required': ['overall', 'per_criterion', 'feedback']\n",
    "    }\n",
    "    try:\n",
    "        validate(instance=response, schema=schema)\n",
    "        return True\n",
    "    except ValidationError as e:\n",
    "        print(f'Output does not match required format: {e}')\n",
    "        return False\n",
    "\n",
    "# Check batch results\n",
    "for r in results:\n",
    "    print(f'Index {r[\"index\"]}:', validate_score_response_format(r['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f65175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 10: Load and normalize chillies/IELTS-writing-task-2-evaluation dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset_name = 'chillies/IELTS-writing-task-2-evaluation'\n",
    "df = load_dataset(dataset_name, split='test').to_pandas()\n",
    "# Normalize columns\n",
    "if 'band' in df.columns:\n",
    "    df = df.rename(columns={'band': 'band_true'})\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = range(len(df))\n",
    "df['word_count'] = df['essay'].astype(str).str.split().map(len)\n",
    "df = df[['id', 'prompt', 'essay', 'band_true', 'word_count']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 11: Batch scoring on the full test set with Qwen2-7B\n",
    "from tqdm import tqdm\n",
    "\n",
    "results = []\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    data = {\n",
    "        'task_type': 'task2',\n",
    "        'essay': row['essay'],\n",
    "        'question': row['prompt'],\n",
    "        'options': {'max_evidence': 2}\n",
    "    }\n",
    "    if validate_score_request(data):\n",
    "        essay, question, _ = preprocess_input(data)\n",
    "        result = score_ielts_task2_qwen2(essay, question, model, tokenizer)\n",
    "        band_pred = result.get('overall', None)\n",
    "    else:\n",
    "        band_pred = None\n",
    "    results.append({'id': row['id'], 'band_true': row['band_true'], 'band_pred': band_pred})\n",
    "\n",
    "df_pred = pd.DataFrame(results)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7accdeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 12: Compute evaluation metrics MAE, RMSE, QWK, Pearson, Spearman\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "\n",
    "def safe_round(x):\n",
    "    try:\n",
    "        return round(float(x))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df_pred = df_pred.dropna(subset=['band_pred']).copy()\n",
    "df_pred['band_pred_round'] = df_pred['band_pred'].map(safe_round)\n",
    "\n",
    "mae = mean_absolute_error(df_pred['band_true'], df_pred['band_pred'])\n",
    "rmse = mean_squared_error(df_pred['band_true'], df_pred['band_pred'], squared=False)\n",
    "qwk = cohen_kappa_score(df_pred['band_true'], df_pred['band_pred_round'], weights='quadratic')\n",
    "pearson = pearsonr(df_pred['band_true'], df_pred['band_pred'])[0]\n",
    "spearman = spearmanr(df_pred['band_true'], df_pred['band_pred'])[0]\n",
    "\n",
    "print(f'MAE: {mae:.4f}')\n",
    "print(f'RMSE: {rmse:.4f}')\n",
    "print(f'QWK: {qwk:.4f}')\n",
    "print(f'Pearson: {pearson:.4f}')\n",
    "print(f'Spearman: {spearman:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb294749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 13: Display comparison table of predictions and ground truth\n",
    "df_pred[['id', 'band_true', 'band_pred', 'band_pred_round']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9806caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 14: Save test set prediction results to file\n",
    "output_path = '/kaggle/working/ielts_task2_predictions.csv'\n",
    "df_pred.to_csv(output_path, index=False)\n",
    "print(f\"Saved predictions to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
